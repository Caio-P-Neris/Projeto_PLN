# -*- coding: utf-8 -*-
"""SentMarket.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1blXhr1amFb9D0T52HdWVicnD2a9WzJkw

# **Processamento de Linguagem Natural [2025-Q3]**
Prof. Alexandre Donizeti Alves

### **PROJETO PR√ÅTICO** [LangChain + Grandes Modelos de Linguagem]

O **PROJETO PR√ÅTICO** deve ser feito utilizando o **Google Colab** com uma conta sua vinculada ao Gmail. O link do seu notebook armazenado no Google Drive e o link de um reposit√≥rio no GitHub devem ser enviados usando o seguinte formul√°rio:

> https://forms.gle/D4gLqP1iGgyn2hbH8


**IMPORTANTE**: A submiss√£o deve ser feita at√© o dia **07/12 (domingo)** APENAS POR UM INTEGRANTE DA EQUIPE, at√© √†s 23h59. Por favor, lembre-se de dar permiss√£o de ACESSO IRRESTRITO para o professor da disciplina.

### **EQUIPE**

---

**POR FAVOR, PREENCHER OS INTEGRANDES DA SUA EQUIPE:**


**Integrante 01:**

`Caio Pereira Neris RA: 11202230288`

**Integrante 02:**

`Matheus Batistelli Vignola RA: 11202230362`

### **GRANDE MODELO DE LINGUAGEM (*Large Language Model - LLM*)**

---

Cada equipe deve selecionar um Grande Modelo de Linguagem (*Large Language Model - LMM*).

Por favor, informe os dados do LLM selecionada:

>


**LLM**:

> ChatGPT (OpenAI)

**Link para a documenta√ß√£o oficial**:
https://platform.openai.com/docs/api-reference/introduction

### **API (Opcional)**
---

Por favor, informe os dados da API selecionada:

**API**: NewsApi

**Site oficial**: https://newsapi.org/

**Link para a documenta√ß√£o oficial**: https://newsapi.org/docs

### **DESCRI√á√ÉO**
---

Implementar um `notebook` no `Google Colab` que fa√ßa uso do framework **`LangChain`** (obrigat√≥rio) e de um **LLM** aplicando, no m√≠nimo, DUAS t√©cnicas de PLN. As t√©cnicas podem ser aplicada em qualquer c√≥rpus obtido a partir de uma **API** ou a partir de uma p√°gina Web.

O **LLM** e a **API** selecionados devem ser informados na seguinte planilha:

> https://docs.google.com/spreadsheets/d/1iIUZcwnywO7RuF6VEJ8Rx9NDT1cwteyvsnkhYr0NWtU/edit?usp=sharing

>
As seguintes t√©cnicas de PLN podem ser usadas:

*   Corre√ß√£o Gramatical
*   Classifica√ß√£o de Textos
*   An√°lise de Sentimentos
*   Detec√ß√£o de Emo√ß√µes
*   Extra√ß√£o de Palavras-chave
*   Tradu√ß√£o de Textos
*   Sumariza√ß√£o de Textos
*   Similaridade de Textos
*   Reconhecimento de Entidades Nomeadas
*   Sistemas de Perguntas e Respostas
>

**IMPORTANTE:** √â obrigat√≥rio usar o e-mail da UFABC.

### **CRIT√âRIOS DE AVALIA√á√ÉO**
---

Ser√£o considerados como crit√©rios de avalia√ß√£o os seguintes pontos:

* Uso do framework **`LangChain`**.

* Escolha e uso de um **LLM**.

* Escolha e uso de uma **API** ou **P√°gina Web**.

* Projeto dispon√≠vel no Github.

* Apresenta√ß√£o (5 a 10 minutos).

* Criatividade no uso do framework **`LangChain`** em conjunto com o **LLM** e a **API**.

**IMPORTANTE**: todo o c√≥digo do notebook deve ser executado. C√≥digo sem execu√ß√£o n√£o ser√° considerado.

### **IMPLEMENTA√á√ÉO: SentMarket**
---

# Neste trabalho, implementamos uma ferramenta de an√°lisa do mercado financeiro por meio de not√≠cias (usando a NewsAPI), possibilitando:



1.   An√°lise de sentimentos:

*   da economia brasileira,
*   sua compara√ß√£o com a economia de outros pa√≠ses,
*   de empresas, setores e a√ß√µes espec√≠ficas  

2.   Classifica√ß√£o de relev√¢ncia e similaridade das not√≠cias para cen√°rio econ√¥mico, extraindo palavras-chaves

3. Sumariza√ß√£o do cen√°rio econ√¥mico

4. Sistema de perguntas e respostas sobre a economia
"""



import os
import requests
import pandas as pd
from datetime import date, timedelta

from openai import OpenAI
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.documents import Document
#from langchain_classic.chains import create_retrieval_chain
#from langchain_classic.chains.combine_documents import create_stuff_documents_chain

import streamlit as st

github_token = st.secrets["GITHUB_TOKEN"]
news_api = st.secrets["NEWS_API"]


# URL base da NewsAPI
url = "https://newsapi.org/v2/everything"

# Mapeamento pa√≠s ‚Üí idioma principal
IDIOMA_POR_PAIS = {
    "brasil": "pt",
    "portugal": "pt",
    "angola": "pt",
    "mo√ßambique": "pt",
    "estados unidos": "en",
    "reino unido": "en",
    "argentina": "es",
    "espanha": "es",
    "m√©xico": "es",
    "fran√ßa": "fr",
    "alemanha": "de",
    "it√°lia": "it",
    "jap√£o": "ja",
    "china": "zh",
    "r√∫ssia": "ru",
    "√≠ndia": "hi",
}

def idioma_do_pais(pais: str):
    pais_lower = pais.lower().strip()
    return IDIOMA_POR_PAIS.get(pais_lower, "en")  # padr√£o: ingl√™s

def pesquisar_noticias(pais):
    termos = [
        f"economia {pais}", f"economia do {pais}",
        "Banco Central", "Central Bank",
        "Selic", "interest rate", "inflation", "PIB", "GDP",
        "Ibovespa", "B3", "financial market", "unemployment"
    ]

    query = " OR ".join([f'"{t}"' for t in termos])

    idioma = idioma_do_pais(pais)

    params = {
        "q": query,
        "language": idioma,
        "sortBy": "relevancy",  # ou "publishedAt" se quiser as mais recentes
        "pageSize": 30, #max √© 100, mas o GPT 4 d√° erro de excesso de tokens
        "apiKey": news_api,
        "from": (date.today() - timedelta(days=7)).isoformat(),
        "to": date.today().isoformat(),
    }
    response = requests.get("https://newsapi.org/v2/everything", params=params)
    data = response.json()

    if data.get("status") != "ok":
        print(f"‚ùå Erro ao buscar not√≠cias: {data.get('message')}")
        return pd.DataFrame()

    noticias = data.get("articles", [])
    articles = pd.DataFrame(noticias)

    if articles.empty:
        print("‚ö†Ô∏è Nenhuma not√≠cia encontrada.")
        return articles

    # Corrige coluna source
    if "source" in articles.columns:
        articles["source"] = articles["source"].apply(lambda s: s.get("name") if isinstance(s, dict) else s)

    # Mant√©m apenas colunas principais
    colunas = ["source", "title", "description", "url", "publishedAt"]
    for col in colunas:
        if col not in articles.columns:
            articles[col] = None

    articles = articles[colunas]

    print(f"Total de not√≠cias encontradas: {len(articles)}")
    return articles

noticias_brasil = pesquisar_noticias("Brasil")


print("Total de resultados:", len(noticias_brasil))

"""https://github.com/marketplace/models/azure-openai/o4-mini"""

from langchain_core.prompts import PromptTemplate
from langchain_openai import ChatOpenAI
import json
import re




# Modelo apontando para o endpoint do GitHub Models
modelo = ChatOpenAI(
    model="openai/gpt-4o",
    temperature=0.1,
    api_key= github_token,
    base_url="https://models.github.ai/inference"
)

def analise_pais(pais, noticias: pd.DataFrame):
    # Converte o DataFrame em string JSON (modelo entende melhor)
    noticias_json = noticias.to_json(orient="records", force_ascii=False)

    # Template do prompt
    prompt = PromptTemplate(
        input_variables=["pais", "dataframe"],
        template=(
            """Voc√™ ir√° receber um dataframe (em formato JSON) com not√≠cias sobre a economia do pa√≠s {pais}.
    Seu objetivo √© devolver um novo dataframe JSON com as colunas:
    - 'source', 'title', 'description', 'url', 'publishedAt'
    - e as novas colunas: 'palavra chave', 'classifica√ß√£o', 'similaridade'

    Etapas:
    1. Leia o t√≠tulo ('title') de cada not√≠cia e classifique todas as n not√≠cias de 1 (mais relevante) a n (menos relevante),
    segundo o crit√©rio: "Quais explicam melhor o cen√°rio econ√¥mico atual e futuro do pa√≠s?".
    2. Para cada t√≠tulo, identifique a 'palavra chave' (termo mais importante que descreve o tema).
    3. Calcule 'similaridade' = n√∫mero de outras not√≠cias que tratam do mesmo tema (mesma palavra chave).
    4. Retorne apenas as 10 not√≠cias mais relevantes (classifica√ß√£o de 1 a 10).
    5. Ap√≥s o dataframe, escreva um breve resumo do cen√°rio econ√¥mico atual do pa√≠s.

    ‚ö†Ô∏è IMPORTANTE: retorne SOMENTE um JSON, com esta estrutura:

    {{
    "data": ...,
    "resumo": "..."
    }}

    Input:
    {{"data": {dataframe}}}

    Considere o pa√≠s: {pais}
    """
        )
    )

    # Cria a cadeia
    chain = prompt | modelo

    # Executa
    resposta = chain.invoke({"pais": pais, "dataframe": noticias.to_string()})

    # Exibe e tenta converter a parte JSON da resposta em DataFrame
    conteudo = resposta.content.strip()

    # üîß Limpeza para remover blocos ```json ... ```
    conteudo_limpo = re.sub(r"^```json\s*|\s*```$", "", conteudo, flags=re.DOTALL).strip()

    try:
        result = json.loads(conteudo_limpo)
        df_resultado = pd.DataFrame(result["data"])
        resumo = result.get("resumo", "")

        # Exibe resumo com melhor formata√ß√£o
        print(f"\nüì∞ **Resumo do cen√°rio econ√¥mico ({pais}):**\n")
        print(resumo.strip())

        return df_resultado, resumo

    except Exception as e:
        print("‚ùå N√£o foi poss√≠vel converter a resposta em DataFrame.")
        print("Erro:", e)
        print("\nConte√∫do limpo recebido:\n", conteudo_limpo[:500], "...")
        return None, None

# Teste
paises = ["Brasil", "Estados Unidos", "Argentina"]

# for pais in paises:
#     analise_pais(pais, pesquisar_noticias(pais))

def pesquisar_acoes(acao):
    """
    Busca not√≠cias recentes sobre uma a√ß√£o (empresa ou ticker) usando a NewsAPI.
    O nome pode ser o ticker (ex: 'PETR4') ou o nome da empresa (ex: 'Petrobras').
    Retorna um DataFrame com as not√≠cias relevantes.
    """

    # Normaliza o nome e gera termos de busca
    termos = [
        f'"{acao}"',                                  # nome direto ou ticker
        f'"a√ß√µes {acao}"',                            # a√ß√µes + nome
        f'"pap√©is da {acao}"',                        # express√£o usada na m√≠dia
        f'"{acao} na B3"',                            # refer√™ncia √† bolsa brasileira
        f'"{acao} stocks"',                           # vers√£o em ingl√™s
        f'"{acao} shares"',                           # a√ß√µes em ingl√™s
        f'"{acao} empresa"',                          # empresa
        f'"{acao} earnings"',                         # resultados trimestrais
        f'"{acao} lucro"',                            # lucro
        f'"{acao} dividendos"',                       # dividendos
        f'"{acao} pre√ßo da a√ß√£o"',                    # pre√ßo
        f'"{acao} cota√ß√£o"',                          # cota√ß√£o
        f'"{acao} mercado"',                           # mercado
        f'"{acao} Brasil"',                           # nome + pa√≠s
        f'"{acao} not√≠cias"',
        f'"{acao} investimento"',
        ]

    # Combina termos com OR
    query = " OR ".join(termos)

    # Escolhe idioma dinamicamente (se a√ß√£o for de empresa brasileira)
    idioma = "pt" if any(x in acao.lower() for x in ["b3", "brasil", "petr", "vale", "itau", "bradesco"]) else "en"

    params = {
        "q": query,
        "language": idioma,
        "sortBy": "publishedAt",
        "pageSize": 50,
        "apiKey": news_api,
        "from": (date.today() - timedelta(days=7)).isoformat(),
        "to": date.today().isoformat(),
    }

    # Requisi√ß√£o √† API
    response = requests.get("https://newsapi.org/v2/everything", params=params)
    data = response.json()

    # Extrai artigos
    articles = data.get("articles", [])
    if not articles:
        print(f"Nenhuma not√≠cia encontrada para {acao}.")
        return pd.DataFrame()

    # Converte em DataFrame
    df = pd.DataFrame(articles)[["source", "title", "description", "url", "publishedAt"]]
    print(f"üîé {len(df)} not√≠cias encontradas para {acao}.")
    return df

# df_petr = pesquisar_acoes("Petrobras")
# df_apple = pesquisar_acoes("Apple")
# df_itau = pesquisar_acoes("Ita√∫ Unibanco")

def analise_acao(acao, noticias: pd.DataFrame):
    # Converte o DataFrame em string JSON (modelo entende melhor)
    noticias_json = noticias.to_json(orient="records", force_ascii=False)

    # Template do prompt
    prompt = PromptTemplate(
        input_variables=["acao", "dataframe"],
        template=(
            """Voc√™ ir√° receber um dataframe (em formato JSON) com not√≠cias sobre a a√ß√£o {acao}.
Seu objetivo √© devolver **apenas um texto** com uma an√°lise de sentimento sobre essa a√ß√£o, com base nas not√≠cias.

No in√≠cio da resposta, coloque:
- Um n√∫mero de 1 a 10 indicando o sentimento geral do mercado.
- Entre par√™nteses, o tipo de sentimento:
  * 1‚Äì4: Pessimismo
  * 5‚Äì7: Neutro
  * 8‚Äì10: Otimismo

Depois disso, escreva um pequeno resumo explicando o motivo dessa avalia√ß√£o.

Input:
{{"noticias": {dataframe}}}

Considere a a√ß√£o: {acao}
"""
        )
    )

    # Cria a cadeia
    chain = prompt | modelo

    # Executa a cadeia e retorna o texto diretamente
    resposta = chain.invoke({"acao": acao, "dataframe": noticias_json})

    print(f"\nüß† An√°lise de sentimento para {acao}:\n")
    #print(resposta.content.strip())
    print("\n")

    return resposta


acoes = ['VALE3', 'Petrobras', 'Banco Ita√∫ SA', 'Apple'] #, 'BRFS3.SA', 'ABEV3.SA', 'WEGE3.SA',
           # 'EGIE3.SA', 'LREN3.SA', 'BBAS3.SA', 'SLCE3.SA' ]

# for acao in acoes:
#     analise_acao(acao, pesquisar_acoes(acao))

def pesquisar_noticias(termos):

    query = " OR ".join([f'"{t}"' for t in termos])

    #idioma = idioma_do_pais(pais)

    params = {
        "q": query,
        "language": "pt", #idioma,
        "sortBy": "relevancy",  # ou "publishedAt" se quiser as mais recentes
        "pageSize": 30, #max √© 100, mas o GPT 4 d√° erro de excesso de tokens
        "apiKey": news_api,
        "from": (date.today() - timedelta(days=7)).isoformat(),
        "to": date.today().isoformat(),
    }
    response = requests.get("https://newsapi.org/v2/everything", params=params)
    data = response.json()

    if data.get("status") != "ok":
        print(f"‚ùå Erro ao buscar not√≠cias: {data.get('message')}")
        return pd.DataFrame()

    noticias = data.get("articles", [])
    articles = pd.DataFrame(noticias)

    if articles.empty:
        print("‚ö†Ô∏è Nenhuma not√≠cia encontrada.")
        return articles

    # Corrige coluna source
    if "source" in articles.columns:
        articles["source"] = articles["source"].apply(lambda s: s.get("name") if isinstance(s, dict) else s)

    # Mant√©m apenas colunas principais
    colunas = ["source", "title", "description", "url", "publishedAt"]
    for col in colunas:
        if col not in articles.columns:
            articles[col] = None

    articles = articles[colunas]

    print(f"Total de not√≠cias encontradas: {len(articles)}")
    return articles

from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.documents import Document

from langchain_core.runnables import RunnableLambda

def df_to_documents(df):
    docs = []
    for _, row in df.iterrows():
        texto = (
            f"T√≠tulo: {row['title']}\n"
            f"Descri√ß√£o: {row.get('description', '')}\n"
            f"Fonte: {row['source']}\n"
            f"URL: {row['url']}\n"
            f"Data: {row['publishedAt']}\n"
        )
        docs.append(Document(page_content=texto))
    return docs



# class SimpleRetriever:
#     def __init__(self, docs):
#         self.docs = docs

#     def get_relevant_documents(self, query):
#         # devolve todas as not√≠cias (sem filtro)
#         return self.docs


system_prompt = (
    '''Voc√™ √© um assistente especializado em economia, finan√ßas e mercados, integrado a uma fun√ß√£o externa de busca de not√≠cias.

Voc√™ agora recebeu o DataFrame com not√≠cias relevantes, voc√™ deve analis√°-lo √† luz da pergunta original do usu√°rio.

Nunca repita a quest√£o interna de busca: responda apenas √† inten√ß√£o expl√≠cita do usu√°rio.

Gerar a resposta final

Use somente as not√≠cias recebidas.

Se o usu√°rio pedir uma an√°lise geral: produza um resumo claro.

Se pedir tend√™ncias: destaque aspectos positivos, negativos ou neutros.

Se pedir previs√£o, avalia√ß√£o ou risco: fa√ßa infer√™ncias baseadas no conte√∫do das not√≠cias (sem inventar dados).

Se o DataFrame vier vazio, diga isso de forma educada e ofere√ßa alternativas.

Restri√ß√µes importantes

Nunca invente not√≠cias, n√∫meros ou fatos.

N√£o fa√ßa chamadas diretas √† API, apenas determine o termo de busca que deve ser usado.

Se houver ambiguidade no pedido do usu√°rio, pe√ßa esclarecimentos antes de sugerir o termo de busca.

Mantenha sempre um tom profissional, claro e voltado a economia/finan√ßas.

Objetivo final
Transformar a pergunta do usu√°rio em:

üîç Uma busca precisa na News API, e depois üìä Uma an√°lise econ√¥mica rigorosa das not√≠cias encontradas.'''
)

prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system_prompt),
        ("system", "Aqui est√£o as not√≠cias relevantes para sua pergunta:\n\n{context}"),
        ("human", "{input}"),
    ]
)


def chatbot_news(pergunta):
    #pergunta = input("Digite sua pergunta: ")

    # 1Ô∏è‚É£ Primeira chamada do GPT ‚Äî extrair o termo de busca
    analise = modelo.invoke(f'''Voc√™ √© um assistente especializado em economia, finan√ßas e mercados, integrado a uma fun√ß√£o externa de busca de not√≠cias.

Agora voc√™ deve apenas identifique claramente qual √© o assunto econ√¥mico solicitado.

O assunto pode ser um pa√≠s, um √≠ndice, um setor, uma a√ß√£o espec√≠fica, uma empresa ou evento macroecon√¥mico.

Extraia apenas termos de busca para utilizar na News API.

Regras obrigat√≥rias:
- Responda SOMENTE com PEQUENOS termos.
- Sem explica√ß√µes.
- Sem par√°grafos.
- Sem listas.
- Sem markdown.
- Sem aspas.

Caso o usu√°rio pe√ßa algo muito amplo (‚Äúcomo est√° a economia‚Äù), concentre a busca nos termos centrais relacionados ao tema. {pergunta}''').content
    print("\nüîé Termo interpretado como relevante:")
    print(analise)

    termo_busca = analise.strip()

    print(f"\nüì° Buscando not√≠cias sobre: {termo_busca} ...")

    # 2Ô∏è‚É£ Buscar not√≠cias
    noticias_df = pesquisar_noticias(termo_busca)

    if noticias_df.empty:
        print("\n‚ùå Nenhuma not√≠cia encontrada.")
        return

    docs = df_to_documents(noticias_df)

    # 3Ô∏è‚É£ Criar o RAG
    #retriever = SimpleRetriever(docs)

    retriever = RunnableLambda(lambda query: docs)

    qa_chain = create_stuff_documents_chain(modelo, prompt)
    rag_chain = create_retrieval_chain(retriever, qa_chain)

    # 4Ô∏è‚É£ Segunda chamada ‚Äî agora sim, responder usando as not√≠cias
    resposta = rag_chain.invoke({"input": pergunta})

    print("\nüß† Resposta baseada nas not√≠cias:")
    print(resposta["answer"])
    return resposta["answer"]
    
#chatbot_news()