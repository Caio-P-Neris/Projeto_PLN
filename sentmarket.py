# -*- coding: utf-8 -*-
"""SentMarket.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1blXhr1amFb9D0T52HdWVicnD2a9WzJkw

# **Processamento de Linguagem Natural [2025-Q3]**
Prof. Alexandre Donizeti Alves

### **PROJETO PR√ÅTICO** [LangChain + Grandes Modelos de Linguagem]

O **PROJETO PR√ÅTICO** deve ser feito utilizando o **Google Colab** com uma conta sua vinculada ao Gmail. O link do seu notebook armazenado no Google Drive e o link de um reposit√≥rio no GitHub devem ser enviados usando o seguinte formul√°rio:

> https://forms.gle/D4gLqP1iGgyn2hbH8


**IMPORTANTE**: A submiss√£o deve ser feita at√© o dia **07/12 (domingo)** APENAS POR UM INTEGRANTE DA EQUIPE, at√© √†s 23h59. Por favor, lembre-se de dar permiss√£o de ACESSO IRRESTRITO para o professor da disciplina.

### **EQUIPE**

---

**POR FAVOR, PREENCHER OS INTEGRANDES DA SUA EQUIPE:**


**Integrante 01:**

`Caio Pereira Neris RA: 11202230288`

**Integrante 02:**

`Matheus Batistelli Vignola RA: 11202230362`

### **GRANDE MODELO DE LINGUAGEM (*Large Language Model - LLM*)**

---

Cada equipe deve selecionar um Grande Modelo de Linguagem (*Large Language Model - LMM*).

Por favor, informe os dados do LLM selecionada:

>


**LLM**:

> ChatGPT (OpenAI)

**Link para a documenta√ß√£o oficial**:
https://platform.openai.com/docs/api-reference/introduction

### **API (Opcional)**
---

Por favor, informe os dados da API selecionada:

**API**: NewsApi

**Site oficial**: https://newsapi.org/

**Link para a documenta√ß√£o oficial**: https://newsapi.org/docs

### **DESCRI√á√ÉO**
---

Implementar um `notebook` no `Google Colab` que fa√ßa uso do framework **`LangChain`** (obrigat√≥rio) e de um **LLM** aplicando, no m√≠nimo, DUAS t√©cnicas de PLN. As t√©cnicas podem ser aplicada em qualquer c√≥rpus obtido a partir de uma **API** ou a partir de uma p√°gina Web.

O **LLM** e a **API** selecionados devem ser informados na seguinte planilha:

> https://docs.google.com/spreadsheets/d/1iIUZcwnywO7RuF6VEJ8Rx9NDT1cwteyvsnkhYr0NWtU/edit?usp=sharing

>
As seguintes t√©cnicas de PLN podem ser usadas:

*   Corre√ß√£o Gramatical
*   Classifica√ß√£o de Textos
*   An√°lise de Sentimentos
*   Detec√ß√£o de Emo√ß√µes
*   Extra√ß√£o de Palavras-chave
*   Tradu√ß√£o de Textos
*   Sumariza√ß√£o de Textos
*   Similaridade de Textos
*   Reconhecimento de Entidades Nomeadas
*   Sistemas de Perguntas e Respostas
>

**IMPORTANTE:** √â obrigat√≥rio usar o e-mail da UFABC.

### **CRIT√âRIOS DE AVALIA√á√ÉO**
---

Ser√£o considerados como crit√©rios de avalia√ß√£o os seguintes pontos:

* Uso do framework **`LangChain`**.

* Escolha e uso de um **LLM**.

* Escolha e uso de uma **API** ou **P√°gina Web**.

* Projeto dispon√≠vel no Github.

* Apresenta√ß√£o (5 a 10 minutos).

* Criatividade no uso do framework **`LangChain`** em conjunto com o **LLM** e a **API**.

**IMPORTANTE**: todo o c√≥digo do notebook deve ser executado. C√≥digo sem execu√ß√£o n√£o ser√° considerado.

### **IMPLEMENTA√á√ÉO: SentMarket**
---

# Neste trabalho, implementamos uma ferramenta de an√°lisa do mercado financeiro por meio de not√≠cias (usando a NewsAPI), possibilitando:



1.   An√°lise de sentimentos:

*   da economia brasileira,
*   sua compara√ß√£o com a economia de outros pa√≠ses,
*   de empresas, setores e a√ß√µes espec√≠ficas  

2.   Classifica√ß√£o de relev√¢ncia e similaridade das not√≠cias para cen√°rio econ√¥mico, extraindo palavras-chaves

3. Sumariza√ß√£o do cen√°rio econ√¥mico

4. Sistema de perguntas e respostas sobre a economia
"""



import os
import requests
import pandas as pd
from datetime import date, timedelta

from openai import OpenAI
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.documents import Document
#from langchain_classic.chains import create_retrieval_chain
#from langchain_classic.chains.combine_documents import create_stuff_documents_chain

#import streamlit as st

github_token = "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" # st.secrets["GITHUB_TOKEN"]
news_api = "66d358d7fc7242d7a1b01c6a3a0e6d1e" #st.secrets["NEWS_API"]


# URL base da NewsAPI
url = "https://newsapi.org/v2/everything"

# Mapeamento pa√≠s ‚Üí idioma principal
IDIOMA_POR_PAIS = {
    "brasil": "pt",
    "portugal": "pt",
    "angola": "pt",
    "mo√ßambique": "pt",
    "estados unidos": "en",
    "reino unido": "en",
    "argentina": "es",
    "espanha": "es",
    "m√©xico": "es",
    "fran√ßa": "fr",
    "alemanha": "de",
    "it√°lia": "it",
    "jap√£o": "ja",
    "china": "zh",
    "r√∫ssia": "ru",
    "√≠ndia": "hi",
}

def idioma_do_pais(pais: str):
    pais_lower = pais.lower().strip()
    return IDIOMA_POR_PAIS.get(pais_lower, "en")  # padr√£o: ingl√™s

def pesquisar_noticias(pais):
    termos = [
        f"economia {pais}", f"economia do {pais}",
        "Banco Central", "Central Bank",
        "Selic", "interest rate", "inflation", "PIB", "GDP",
        "Ibovespa", "B3", "financial market", "unemployment"
    ]

    query = " OR ".join([f'"{t}"' for t in termos])

    idioma = idioma_do_pais(pais)

    params = {
        "q": query,
        "language": idioma,
        "sortBy": "relevancy",  # ou "publishedAt" se quiser as mais recentes
        "pageSize": 30, #max √© 100, mas o GPT 4 d√° erro de excesso de tokens
        "apiKey": news_api,
        "from": (date.today() - timedelta(days=7)).isoformat(),
        "to": date.today().isoformat(),
    }
    response = requests.get("https://newsapi.org/v2/everything", params=params)
    data = response.json()

    if data.get("status") != "ok":
        print(f"‚ùå Erro ao buscar not√≠cias: {data.get('message')}")
        return pd.DataFrame()

    noticias = data.get("articles", [])
    articles = pd.DataFrame(noticias)

    if articles.empty:
        print("‚ö†Ô∏è Nenhuma not√≠cia encontrada.")
        return articles

    # Corrige coluna source
    if "source" in articles.columns:
        articles["source"] = articles["source"].apply(lambda s: s.get("name") if isinstance(s, dict) else s)

    # Mant√©m apenas colunas principais
    colunas = ["source", "title", "description", "url", "publishedAt"]
    for col in colunas:
        if col not in articles.columns:
            articles[col] = None

    articles = articles[colunas]

    print(f"Total de not√≠cias encontradas: {len(articles)}")
    return articles

noticias_brasil = pesquisar_noticias("Brasil")


print("Total de resultados:", len(noticias_brasil))

"""https://github.com/marketplace/models/azure-openai/o4-mini"""

from langchain_core.prompts import PromptTemplate
from langchain_openai import ChatOpenAI
import json
import re




# Modelo apontando para o endpoint do GitHub Models
modelo = ChatOpenAI(
    model="openai/gpt-4o",
    temperature=0.1,
    api_key= github_token,
    base_url="https://models.github.ai/inference"
)
def analise_pais(pais, noticias: pd.DataFrame):
    # Converte o DataFrame em string JSON (modelo entende melhor)
    noticias_json = noticias.to_json(orient="records", force_ascii=False)

    # Template do prompt
    prompt = PromptTemplate(
        input_variables=["pais", "dataframe"],
        template=(
            """Voc√™ ir√° receber um dataframe (em formato JSON) com not√≠cias sobre a economia do pa√≠s {pais}.
    Seu objetivo √© devolver um novo dataframe JSON com as colunas:
    - 'source', 'title', 'description', 'url', 'publishedAt'
    - e as novas colunas: 'palavra chave', 'classifica√ß√£o', 'similaridade'

    Etapas:
    1. Leia o t√≠tulo ('title') de cada not√≠cia e classifique todas as n not√≠cias de 1 (mais relevante) a n (menos relevante),
    segundo o crit√©rio: "Quais explicam melhor o cen√°rio econ√¥mico atual e futuro do pa√≠s?".
    2. Para cada t√≠tulo, identifique a 'palavra chave' (termo mais importante que descreve o tema).
    3. Calcule 'similaridade' = n√∫mero de outras not√≠cias que tratam do mesmo tema (mesma palavra chave).
    4. Retorne apenas as 10 not√≠cias mais relevantes (classifica√ß√£o de 1 a 10).
    5. Ap√≥s o dataframe, escreva um breve resumo do cen√°rio econ√¥mico atual do pa√≠s.

    ‚ö†Ô∏è IMPORTANTE: retorne SOMENTE um JSON, com esta estrutura:

    {{
    "data": ...,
    "resumo": "..."
    }}

    Input:
    {{"data": {dataframe}}}

    Considere o pa√≠s: {pais}
    """
        )
    )

    # Cria a cadeia
    chain = prompt | modelo

    # Executa
    resposta = chain.invoke({"pais": pais, "dataframe": noticias.to_string()})

    # Exibe e tenta converter a parte JSON da resposta em DataFrame
    conteudo = resposta.content.strip()

    # üîß Limpeza para remover blocos ```json ... ```
    conteudo_limpo = re.sub(r"^```json\s*|\s*```$", "", conteudo, flags=re.DOTALL).strip()

    try:
        result = json.loads(conteudo_limpo)
        df_resultado = pd.DataFrame(result["data"])
        resumo = result.get("resumo", "")

        # Exibe resumo com melhor formata√ß√£o
        print(f"\nüì∞ **Resumo do cen√°rio econ√¥mico ({pais}):**\n")
        print(resumo.strip())

        return df_resultado, resumo

    except Exception as e:
        print("‚ùå N√£o foi poss√≠vel converter a resposta em DataFrame.")
        print("Erro:", e)
        print("\nConte√∫do limpo recebido:\n", conteudo_limpo[:500], "...")
        return None, None

# Teste
paises = ["Brasil", "Estados Unidos", "Argentina"]

# for pais in paises:
#     analise_pais(pais, pesquisar_noticias(pais))

def pesquisar_acoes(acao):
    """
    Busca not√≠cias recentes sobre uma a√ß√£o (empresa ou ticker) usando a NewsAPI.
    O nome pode ser o ticker (ex: 'PETR4') ou o nome da empresa (ex: 'Petrobras').
    Retorna um DataFrame com as not√≠cias relevantes.
    """

    # Normaliza o nome e gera termos de busca
    termos = [
        f'"{acao}"',                                  # nome direto ou ticker
        f'"a√ß√µes {acao}"',                            # a√ß√µes + nome
        f'"pap√©is da {acao}"',                        # express√£o usada na m√≠dia
        f'"{acao} na B3"',                            # refer√™ncia √† bolsa brasileira
        f'"{acao} stocks"',                           # vers√£o em ingl√™s
        f'"{acao} shares"',                           # a√ß√µes em ingl√™s
        f'"{acao} empresa"',                          # empresa
        f'"{acao} earnings"',                         # resultados trimestrais
        f'"{acao} lucro"',                            # lucro
        f'"{acao} dividendos"',                       # dividendos
        f'"{acao} pre√ßo da a√ß√£o"',                    # pre√ßo
        f'"{acao} cota√ß√£o"',                          # cota√ß√£o
        f'"{acao} mercado"',                           # mercado
        f'"{acao} Brasil"',                           # nome + pa√≠s
        f'"{acao} not√≠cias"',
        f'"{acao} investimento"',
        ]

    # Combina termos com OR
    query = " OR ".join(termos)

    # Escolhe idioma dinamicamente (se a√ß√£o for de empresa brasileira)
    idioma = "pt" if any(x in acao.lower() for x in ["b3", "brasil", "petr", "vale", "itau", "bradesco"]) else "en"

    params = {
        "q": query,
        "language": idioma,
        "sortBy": "publishedAt",
        "pageSize": 50,
        "apiKey": news_api,
        "from": (date.today() - timedelta(days=7)).isoformat(),
        "to": date.today().isoformat(),
    }

    # Requisi√ß√£o √† API
    response = requests.get("https://newsapi.org/v2/everything", params=params)
    data = response.json()

    # Extrai artigos
    articles = data.get("articles", [])
    if not articles:
        print(f"Nenhuma not√≠cia encontrada para {acao}.")
        return pd.DataFrame()

    # Converte em DataFrame
    df = pd.DataFrame(articles)[["source", "title", "description", "url", "publishedAt"]]
    print(f"üîé {len(df)} not√≠cias encontradas para {acao}.")
    return df

# df_petr = pesquisar_acoes("Petrobras")
# df_apple = pesquisar_acoes("Apple")
# df_itau = pesquisar_acoes("Ita√∫ Unibanco")

def analise_acao(acao, noticias: pd.DataFrame):
    # Converte o DataFrame em string JSON (modelo entende melhor)
    noticias_json = noticias.to_json(orient="records", force_ascii=False)

    # Template do prompt
    prompt = PromptTemplate(
        input_variables=["acao", "dataframe"],
        template=(
            """Voc√™ ir√° receber um dataframe (em formato JSON) com not√≠cias sobre a a√ß√£o {acao}.
Seu objetivo √© devolver **apenas um texto** com uma an√°lise de sentimento sobre essa a√ß√£o, com base nas not√≠cias.

No in√≠cio da resposta, coloque:
- Um n√∫mero de 1 a 10 indicando o sentimento geral do mercado.
- Entre par√™nteses, o tipo de sentimento:
  * 1‚Äì4: Pessimismo
  * 5‚Äì7: Neutro
  * 8‚Äì10: Otimismo

Depois disso, escreva um pequeno resumo explicando o motivo dessa avalia√ß√£o.

Input:
{{"noticias": {dataframe}}}

Considere a a√ß√£o: {acao}
"""
        )
    )

    # Cria a cadeia
    chain = prompt | modelo

    # Executa a cadeia e retorna o texto diretamente
    resposta = chain.invoke({"acao": acao, "dataframe": noticias_json})

    print(f"\nüß† An√°lise de sentimento para {acao}:\n")
    #print(resposta.content.strip())
    print("\n")

    return resposta


acoes = ['VALE3', 'Petrobras', 'Banco Ita√∫ SA', 'Apple'] #, 'BRFS3.SA', 'ABEV3.SA', 'WEGE3.SA',
           # 'EGIE3.SA', 'LREN3.SA', 'BBAS3.SA', 'SLCE3.SA' ]

# for acao in acoes:
#     analise_acao(acao, pesquisar_acoes(acao))

import langchain
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.documents import Document
from langchain_core.runnables import RunnableLambda

def pesquisar_noticias_por_termo(termos):

    query = " OR ".join([f'"{t}"' for t in termos])

    #idioma = idioma_do_pais(pais)

    params = {
        "q": query,
        "language": "pt", #idioma,
        "sortBy": "relevancy",  # ou "publishedAt" se quiser as mais recentes
        "pageSize": 30, #max √© 100, mas o GPT 4 d√° erro de excesso de tokens
        "apiKey": news_api,
        "from": (date.today() - timedelta(days=7)).isoformat(),
        "to": date.today().isoformat(),
    }
    response = requests.get("https://newsapi.org/v2/everything", params=params)
    data = response.json()

    if data.get("status") != "ok":
        print(f"‚ùå Erro ao buscar not√≠cias: {data.get('message')}")
        return pd.DataFrame()

    noticias = data.get("articles", [])
    articles = pd.DataFrame(noticias)

    if articles.empty:
        print("‚ö†Ô∏è Nenhuma not√≠cia encontrada.")
        return articles

    # Corrige coluna source
    if "source" in articles.columns:
        articles["source"] = articles["source"].apply(lambda s: s.get("name") if isinstance(s, dict) else s)

    # Mant√©m apenas colunas principais
    colunas = ["source", "title", "description", "url", "publishedAt"]
    for col in colunas:
        if col not in articles.columns:
            articles[col] = None

    articles = articles[colunas]

    print(f"Total de not√≠cias encontradas: {len(articles)}")
    return articles

#================================================================================================================================

def df_to_documents(df):
    docs = []
    for _, row in df.iterrows():
        # Adicionei tratamento para caso description seja None
        desc = row.get('description', '')
        if desc is None: desc = ""

        texto = (
            f"T√≠tulo: {row['title']}\n"
            f"Descri√ß√£o: {desc}\n"
            f"Fonte: {row['source']}\n"
            f"Data: {row['publishedAt']}\n"
        )
        docs.append(Document(page_content=texto))
    return docs

# --- PROMPT 1: APENAS PARA O ANALISTA (RAG) ---
# Sem instru√ß√µes de busca aqui, foco apenas na an√°lise.
prompt_analista_sistema = (
    """Voc√™ √© um analista econ√¥mico s√™nior.
    Sua tarefa √© responder √† pergunta do usu√°rio utilizando as not√≠cias fornecidas como base principal (Contexto).

    Diretrizes:
    1. **Prioridade:** Use os fatos apresentados nas not√≠cias (Contexto) para fundamentar sua resposta. Cite as fontes quando poss√≠vel.
    2. **Conhecimento H√≠brido:** Se as not√≠cias fornecidas forem insuficientes, vagas ou muito antigas, VOC√ä DEVE complementar a resposta com seu pr√≥prio conhecimento sobre economia e finan√ßas.
    3. **Transpar√™ncia:** Se usar seu conhecimento pr√≥prio, deixe claro (ex: "Embora as not√≠cias n√£o mencionem, historicamente...").
    4. **Formato:** Seja direto, profissional e estruturado.

    Se as not√≠cias forem totalmente irrelevantes para o tema, ignore-as e responda com seu conhecimento, avisando o usu√°rio que as not√≠cias recentes n√£o ajudaram.
    """
)

prompt_rag = ChatPromptTemplate.from_messages(
    [
        ("system", prompt_analista_sistema),
        ("system", "CONTEXTO (Not√≠cias Recentes):\n\n{context}"),
        ("human", "{input}"),
    ]
)

#================================================================================================================================

def chatbot_news(pergunta):
    #pergunta = input("Digite sua pergunta: ")

    # Aqui usamos um prompt "inline" simples, pois √© uma tarefa r√°pida
    prompt_busca = f"""
    Aja como um extrator de palavras-chave para uma API de not√≠cias.
    Analise a pergunta: "{pergunta}"

    Retorne APENAS os termos principais para busca (ex: "Infla√ß√£o Brasil", "Apple stock", "Taxa Selic").
    Se for sobre um pa√≠s, coloque o nome do pa√≠s, mas sempre junto do tema. Cada termo deve ser um t√≥pico completo (ex: "desemprego brasil"). N√£o retorne palavras isoladas como "Brasil", "economia", etc. N√£o use frases longas.
    Sa√≠da SEMPRE no formato: termo1, termo2, termo3

    N√£o use pontua√ß√£o, n√£o use aspas, apenas texto cru.
    """

    termo_busca = modelo.invoke(prompt_busca).content.strip()

    print(f"\nüîé Termo interpretado: {termo_busca}")
    print(f"üì° Buscando not√≠cias...")

    #Buscar not√≠cias
    try:
        noticias_df = pesquisar_noticias_por_termo(termo_busca)
    except Exception as e:
        print(f"Erro na busca: {e}")
        return

    # Se n√£o achar not√≠cias, o modelo ainda pode tentar responder
    docs = []
    if not noticias_df.empty:
        docs = df_to_documents(noticias_df)
        print(f"‚úÖ {len(docs)} not√≠cias encontradas e enviadas ao modelo.")
    else:
        print("‚ö†Ô∏è Nenhuma not√≠cia encontrada. O modelo responder√° apenas com conhecimento pr√©vio.")

    # O truque do RunnableLambda para injetar os docs que j√° temos
    retriever = RunnableLambda(lambda x: docs)

    qa_chain = create_stuff_documents_chain(modelo, prompt_rag)
    rag_chain = create_retrieval_chain(retriever, qa_chain)

    #Gerar Resposta Final
    print("\nüß† Gerando an√°lise...")
    resultado = rag_chain.invoke({"input": pergunta})

    print("\n" + "="*40)
    print(resultado["answer"])
    print("="*40 + "\n")
    return resultado["answer"]